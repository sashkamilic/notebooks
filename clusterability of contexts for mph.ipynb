{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterability pilot\n",
    "\n",
    "From Barend's email:\n",
    "\n",
    "''\n",
    "Take the ~1290 homonyms, polysemes, and monosemes.\n",
    "\n",
    "initialize a dictionary clusterability = {}\n",
    "\n",
    "Go over SUBTL (if you don't have it, I can upload it to the server) and extract 100 random instances of sentences per word.\n",
    "\n",
    "For every word w in the 1290 test items\n",
    "\n",
    "-- for every sentence in the 100 sentences, extract the sentence vector by summing over all the content word vectors according to word2vec (or glove -- i forget which one was better)\n",
    "\n",
    "-- initialize scores = []\n",
    "\n",
    "-- for k in {2,5,10,20,50}, \n",
    "\n",
    "---- run k-means clustering on the 100 sentence vectors\n",
    "\n",
    "---- calculate the average silhouette for each vector and add this value to scores https://en.wikipedia.org/wiki/Silhouette_(clustering)\n",
    "\n",
    "-- set clusterability[w] to be the max of scores\n",
    "\n",
    "Compare the clusterability scores for the three groups of words: are homonyms more clusterable than polysemes and monosemes etc. (You can use a t-test for this)\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import progressbar\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "import random\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBTITLE_CORPUS = '/ais/clspace5/u/sasa/HOMONOMY_POLYSEMY/Subtlex/Subtlex.US.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim Continuous Skipgram\n",
    "# English Wikipedia Dump of February 2017 Gigaword 5th Edition\n",
    "# not lemmatized\n",
    "VECTORS = KeyedVectors.load_word2vec_format('/u/sasa/sasa/model.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1287"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOMO = [line.split('\\t')[0] for line in open('/u/sasa/homonymy_polysemy/Sasa_SOS.HomonymHybrid.tsv').readlines()[1:]]\n",
    "POLY = [line.split('\\t')[0] for line in open('/u/sasa/homonymy_polysemy/Sasa_SOS.Polysemes.tsv').readlines()[1:]]\n",
    "MONO = [line.split('\\t')[0] for line in open('/u/sasa/homonymy_polysemy/Sasa_SOS.Monosemes.tsv').readlines()[1:]]\n",
    "len(HOMO + POLY + MONO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_clusterability(words, context_dict):\n",
    "    '''\n",
    "    Returns a dict mapping between target words in `words`\n",
    "    and a clusterability score.\n",
    "    \n",
    "    words - iterable of target words (strings)\n",
    "    context-dict - maps between target word t\n",
    "        and an iterable containing t's context words (strings) \n",
    "    '''\n",
    "\n",
    "    clusterability = {}\n",
    "\n",
    "    bar = progressbar.ProgressBar(maxval=len(HOMO + POLY + MONO))\n",
    "    bar.start()\n",
    "    for i,tw in enumerate(words):\n",
    "        bar.update(i)\n",
    "        clusterability_w = []\n",
    "        \n",
    "        contexts = context_dict[tw]\n",
    "        context_vectors = []\n",
    "        \n",
    "        if type(contexts[0]) == iter:\n",
    "            # we are clustering sentence vectors\n",
    "            for sentence in contexts:\n",
    "                # remove stopwords *and* target word\n",
    "                sentence = set(sentence.strip().split()) - (stopWords | set([tw]))\n",
    "                if not sentence:\n",
    "                    # sentence contained only stopwords and target word, thus empty\n",
    "                    continue\n",
    "                context_vectors.append(np.sum([VECTORS[cw] for cw in sentence if cw in VECTORS], axis=0))\n",
    "                # filter out \"empty\" sentence vectors\n",
    "                context_vectors = [sv for sv in sentence_vectors if sv.shape]\n",
    "\n",
    "        elif type(contexts[0]) == str:\n",
    "            context_vectors = [VECTORS[cw] for cw in context_dict[tw] if cw in VECTORS]\n",
    "            \n",
    "        else:\n",
    "            raise TypeError('contexts should be strings or lists of strings')\n",
    "\n",
    "        # for k in {2,5,10,20,50}, ---- run k-means clustering on the context vectors\n",
    "        scores = []\n",
    "        for k in range(2, min(52, len(context_vectors)), 2):\n",
    "            kmeans = KMeans(init='k-means++', n_clusters=k)\n",
    "            #print(len(sentence_vectors))\n",
    "            #print([v.shape for v in sentence_vectors])\n",
    "            kmeans.fit(context_vectors)\n",
    "            score = silhouette_score(context_vectors, kmeans.labels_, metric='euclidean')\n",
    "            scores.append(score)\n",
    "\n",
    "        clusterability[tw] = np.mean(max(scores))\n",
    "\n",
    "    bar.finish()\n",
    "    \n",
    "    return clusterability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading sublex file [1/3]...\n",
      "Reading sublex file [2/3]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading sublex file [3/3]...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50619248"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find most strongy associated context words for each w in MONO/POLY/HOMO using pmi\n",
    "\n",
    "# (target_word, context_word) frequencies\n",
    "f_xy = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "target_words = set(HOMO + POLY + MONO)\n",
    "\n",
    "print('Reading sublex file [1/3]...')\n",
    "with open(SUBTITLE_CORPUS, \"r\", encoding='latin-1') as f:\n",
    "    bar = progressbar.ProgressBar(maxval=len(f.readlines()))\n",
    "\n",
    "print('Reading sublex file [2/3]...')\n",
    "bar.start()\n",
    "with open(SUBTITLE_CORPUS, \"r\", encoding='latin-1') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        sentence = tokenizer.tokenize(line.lower())\n",
    "        for tw in target_words & set(sentence):\n",
    "            for cw in set(sentence):\n",
    "                if cw == tw:\n",
    "                    continue\n",
    "                f_xy[tw][cw] += 1\n",
    "        bar.update(i)\n",
    "bar.finish()\n",
    "\n",
    "print('Reading sublex file [3/3]...')\n",
    "with open(SUBTITLE_CORPUS, \"r\", encoding='latin-1') as f:\n",
    "    # all word frequencies\n",
    "    f_x = Counter(tokenizer.tokenize(f.read().lower()))\n",
    "                  \n",
    "print('Done!')\n",
    "\n",
    "# total number of words (needed in PMI computation)\n",
    "N = sum(f_x.values())\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ranked (by PMI) context words per target\n",
    "Note that *not* having a lower threshold on the frequency of cooccurance (between target word and context word) produces pretty bad context word rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_words_pmi = {}\n",
    "for tw in HOMO + POLY + MONO:\n",
    "    cws = {}\n",
    "    for cw, f in f_xy[tw].items():\n",
    "        # TODO: what should min freq threshold be?\n",
    "        #if f_xy[tw][cw] < 3:\n",
    "        #    continue\n",
    "        if f_x[cw] / N < 2e-6:\n",
    "            continue\n",
    "        cws[cw] = np.log(N * f / (f_x[tw] * f_x[cw]))\n",
    "    # sort words by pmi\n",
    "    cws = sorted(cws.items(), key=lambda t:t[1], reverse=True)\n",
    "    context_words_pmi[tw] = cws\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some randomly selected target words and their top context words. Note: not all words (e.g. \"sash\") contain enough context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APARTMENT\n",
      "vacant, rented, renting, lease, samuels, tenant, coronary, bugged, crappy, penthouse, loft, tenants, complex, landlady, decorated, trend, drapes, berger, doorman, shitty\n",
      "LINKS\n",
      "cuff, hmmm, lunar, mainframe, defenses, andrews, tile, website, neural, mediterranean, rommel, curl, weaker, conditioned, und, edison, platinum, patti, ranking, supplied\n",
      "BEARDED\n",
      "edwardes, furry, clam, jumpin, haste, shaved, skiing, shortage, panther, dancers, seals, strap, overlook, evidently, oyster, singers, suited, portland, enters, bikini\n",
      "BURN\n",
      "slash, crisp, amulet, fires, perish, brighter, bridges, shred, villages, rubber, plantation, initiate, flame, crops, scars, circuits, stake, flames, palms, marks\n",
      "TWIRL\n",
      "baton, hawaiian, obnoxious, whooping, salsa, spine, checkbook, vacant, knob, knack, tame, viola, alarms, ½, shadows, massage, umbrella, wink, arch, rum\n",
      "SNOW\n",
      "cone, sleigh, dashing, lotus, globe, melted, eileen, shovel, frosty, skiing, footprints, avalanche, barefoot, driven, duel, sled, snowing, edwardes, crisp, winter\n",
      "RIP\n",
      "offs, roaring, knob, throats, apart, clapping, cashed, toilets, habitat, shits, assed, bare, broccoli, stomp, elbows, grip, gigantic, throat, insides, lungs\n",
      "STICK\n",
      "thai, limbo, needle, necks, needles, grin, tongue, gum, glue, carrot, dynamite, measuring, fork, plaster, budge, exhaust, mud, peeing, thumb, mailbox\n",
      "SASH\n",
      "satin, weights, sleeves, badges, ankles, wraps, miniature, earring, ropes, cinderella, unite, applications, cologne, sleigh, tying, hopkins, wink, collar, strangled, rosa\n"
     ]
    }
   ],
   "source": [
    "random.seed(2018)\n",
    "examples = random.sample(MONO, 3) + random.sample(POLY, 3) + random.sample(HOMO, 3)\n",
    "for tw in examples:\n",
    "    print(tw.upper())\n",
    "    print(', '.join([t[0] for t in context_words_pmi[tw][:20] if t[0] not in stopWords]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175407"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test with different kinds of contexts\n",
    "1. randomly selected sentence vectors\n",
    "2. randomly selected context words\n",
    "3. top-k context words by pmi (k=10,20,30,50,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly selected sentences\n",
    "with open(os.path.join(CONTEXT_DIR, '{}.txt'.format(tw))) as f_context:\n",
    "            contexts = np.random.choice(f_context.readlines(), size=100)\n",
    "        \n",
    "        sentence_vectors = []\n",
    "        for sentence in contexts:\n",
    "            # remove stopwords *and* target word\n",
    "            sentence = set(sentence.strip().split()) - (stopWords | set([tw]))\n",
    "            if not sentence:\n",
    "                # sentence contained only stopwords and target word, thus empty\n",
    "                continue\n",
    "            sentence_vectors.append(np.sum([VECTORS[cw] for cw in sentence if cw in VECTORS], axis=0))\n",
    "        # filter out \"empty\" sentence vectors\n",
    "        sentence_vectors = [sv for sv in sentence_vectors if sv.shape]\n",
    "        if len(sentence_vectors) < 50:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=1287, minmax=(141, 5528), mean=1258.1802641802642, variance=1140357.993918126, skewness=1.3523214304347755, kurtosis=1.308854857119539)\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "contexts = [[t[0] for t in l if t[0] not in stopWords] for l in context_words_pmi.values()]\n",
    "\n",
    "from scipy.stats import describe\n",
    "#print([len(x) for x in contexts])\n",
    "num_contexts = list([len(list(x)) for x in contexts])\n",
    "print(describe(num_contexts))\n",
    "print(len([x for x in num_contexts if x < 50]))\n",
    "print(len([x for x in num_contexts if x < 20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-10 context words by pmi score\n",
      "mono\n",
      "0.080 (0.039)\n",
      "poly\n",
      "0.082 (0.043)\n",
      "homo\n",
      "0.083 (0.041)\n",
      "Ttest_indResult(statistic=-0.5604315933798681, pvalue=0.5753317317476934)\n",
      "Ttest_indResult(statistic=-1.2244162038759583, pvalue=0.22113212246744604)\n",
      "Ttest_indResult(statistic=-0.6199053597944446, pvalue=0.535485073361547)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-20 context words by pmi score\n",
      "mono\n",
      "0.070 (0.027)\n",
      "poly\n",
      "0.073 (0.032)\n",
      "homo\n",
      "0.073 (0.033)\n",
      "Ttest_indResult(statistic=-1.3623398144447478, pvalue=0.17344901131194018)\n",
      "Ttest_indResult(statistic=-1.4427505061451524, pvalue=0.1494565722203723)\n",
      "Ttest_indResult(statistic=-0.0973409297017433, pvalue=0.9224784395276681)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-30 context words by pmi score\n",
      "mono\n",
      "0.066 (0.024)\n",
      "poly\n",
      "0.068 (0.024)\n",
      "homo\n",
      "0.068 (0.028)\n",
      "Ttest_indResult(statistic=-1.0610628938910691, pvalue=0.288960596497583)\n",
      "Ttest_indResult(statistic=-1.0939405115084297, pvalue=0.2742888047376381)\n",
      "Ttest_indResult(statistic=-0.11072209004498942, pvalue=0.9118626824774975)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-50 context words by pmi score\n",
      "mono\n",
      "0.062 (0.018)\n",
      "poly\n",
      "0.063 (0.018)\n",
      "homo\n",
      "0.064 (0.024)\n",
      "Ttest_indResult(statistic=-0.6966368027790235, pvalue=0.4862192432148398)\n",
      "Ttest_indResult(statistic=-1.469305021150172, pvalue=0.14211748439214836)\n",
      "Ttest_indResult(statistic=-0.868392875933672, pvalue=0.38542268884806197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98% |####################################################################### |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-100 context words by pmi score\n",
      "mono\n",
      "0.058 (0.013)\n",
      "poly\n",
      "0.059 (0.013)\n",
      "homo\n",
      "0.060 (0.016)\n",
      "Ttest_indResult(statistic=-0.40949156783689905, pvalue=0.682281398764693)\n",
      "Ttest_indResult(statistic=-1.2679884497021294, pvalue=0.2051468471089382)\n",
      "Ttest_indResult(statistic=-0.9107806129136616, pvalue=0.3626673751544677)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r\n"
     ]
    }
   ],
   "source": [
    "# top-k context words\n",
    "for k in [10, 20, 30, 50, 100]:\n",
    "    context_dict = {}\n",
    "    for tw in context_words_pmi.keys():\n",
    "        cws = [t[0] for t in context_words_pmi[tw] if t[0] not in stopWords and t[0] in VECTORS]\n",
    "        if len(cws) < k:\n",
    "            print(\"{} has {}<k={} context words; it will not be included in dataset\".format(tw, len(cws), k))\n",
    "            continue\n",
    "        context_dict[tw] = cws[:k]\n",
    "\n",
    "    #words = MONO + POLY + HOMO\n",
    "    clusterability = context_clusterability(context_dict.keys(), context_dict)\n",
    "    print(\"top-{} context words by pmi score\".format(k))\n",
    "    print_results(clusterability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(clusterability):\n",
    "    scores = {\n",
    "        'homo': [],\n",
    "        'poly': [],\n",
    "        'mono': []\n",
    "    }\n",
    "\n",
    "    for type_,l in zip(['mono', 'poly', 'homo'], [MONO, POLY, HOMO]):\n",
    "        for w in l:\n",
    "            if w in clusterability:\n",
    "                scores[type_].append(clusterability[w])\n",
    "        print(type_)\n",
    "        print(\"{:.3f} ({:.3f})\".format(np.mean(scores[type_]), np.std(scores[type_])))\n",
    "\n",
    "    print(stats.ttest_ind(scores['mono'], scores['poly']))\n",
    "    print(stats.ttest_ind(scores['mono'], scores['homo']))\n",
    "    print(stats.ttest_ind(scores['poly'], scores['homo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
